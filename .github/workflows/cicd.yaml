name: CICD Pipeline

on:
  push:
    branches:
      - master  # or 'main' if that's your default branch

jobs:
  model-deployment:
    runs-on: ubuntu-latest

    steps:
      # âœ… Step 1: Checkout repository
      - name: Checkout code
        uses: actions/checkout@v3

      # âœ… Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # âœ… Step 3: Cache pip dependencies for faster builds
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Upgrade pip and install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      # ðŸŸ¡ Step 4: Detect if DVC pipeline files have changed between commits
      # ðŸ”¸ Added this step to avoid re-running the pipeline unnecessarily for unrelated changes
      - name: Check for pipeline changes
        id: check_pipeline
        run: |
          git fetch origin ${{ github.ref }}
          CHANGED_FILES=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }})
          echo "Changed files:"
          echo "$CHANGED_FILES"
          if echo "$CHANGED_FILES" | grep -E '(^dvc\.yaml$|^dvc\.lock$|^params\.yaml$|^data/|^src/)'; then
            echo "run_dvc=true" >> $GITHUB_ENV
          else
            echo "run_dvc=false" >> $GITHUB_ENV
          fi

      # âœ… Step 5: Run DVC pipeline only when pipeline-relevant files changed
      # ðŸ”¸ This prevents redundant S3 uploads & retraining for every push
      - name: Run DVC Pipeline
        if: env.run_dvc == 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-north-1
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        run: |
          # Optional: skip repro if hashes are unchanged
          if dvc status -c | grep -q "Data and pipelines are up to date."; then
            echo "No DVC changes detected. Skipping repro."
          else
            dvc repro
          fi

      # âœ… Step 6: Push DVC-tracked data to remote only when pipeline ran
      - name: Push DVC-tracked data to remote
        if: env.run_dvc == 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-north-1
        run: |
          dvc push

      # âœ… Step 7: Configure Git for committing dvc.lock changes
      - name: Configure Git
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"

      # âœ… Step 8: Commit only relevant DVC changes
      # ðŸ”¸ Changed to commit ONLY if dvc.lock or pipeline files changed, avoiding empty commits
      - name: Commit and Push DVC Lockfile Changes
        if: env.run_dvc == 'true'
        run: |
          git add dvc.lock
          git diff --quiet && git diff --staged --quiet || git commit -m "ðŸ”„ Update dvc.lock after pipeline run"
          git push origin ${{ github.ref_name }}

      # âœ… Step 9: Install test dependencies (unchanged)
      - name: Install test dependencies
        run: |
          pip install pytest

      # âœ… Step 10: Run model loading test (unchanged)
      - name: Run model loading test
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-north-1
        run: |
          pytest scripts/test_model_loading.py
      - name: Run model signature test
        run: pytest scripts/test_model_signature.py

